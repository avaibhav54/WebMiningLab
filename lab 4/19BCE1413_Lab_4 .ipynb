{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef936167",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7e2e806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff36033b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aayus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aayus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "317d156b",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=[\n",
    "    \"doc1.TXT\",\n",
    "    \"doc2.TXT\",\n",
    "    \"doc3.TXT\",\n",
    "    \"doc4.TXT\",\n",
    "    \"doc5.TXT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "598551a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    s=text\n",
    "    s=s.lower()\n",
    "    s=s.replace('[A-Za-z0-9]/g','')\n",
    "    s=s.strip()\n",
    "    words=nltk.word_tokenize(s)\n",
    "    stop_words=set(stopwords.words('english'))\n",
    "    words=[word for word in words if word not in stop_words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d931a8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Retrieval Systems is used with database systems\n",
      "Information is in Storage Storage\n",
      "Digital Speech systems can be used in Synthesis and Systems\n",
      "Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\n",
      "Database Management system is used for storage storage\n"
     ]
    }
   ],
   "source": [
    "for(i,doc) in enumerate(document):\n",
    "    f = open(doc, \"r\")\n",
    "    texts=f.read().strip()\n",
    "    print(texts)\n",
    "    words=preprocess(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e19fee",
   "metadata": {},
   "source": [
    "# Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c56fe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize , word_tokenize\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "Stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15d1e1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=[\n",
    "    \"doc1.TXT\",\n",
    "    \"doc2.TXT\",\n",
    "    \"doc3.TXT\",\n",
    "    \"doc4.TXT\",\n",
    "    \"doc5.TXT\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81ea8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self ,docId, freq = None):\n",
    "        self.freq = freq\n",
    "        self.doc = docId\n",
    "        self.nextval = None\n",
    "    \n",
    "class SlinkedList:\n",
    "    def __init__(self ,head = None):\n",
    "        self.head = head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c79d5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finding_all_unique_words_and_freq(words):\n",
    "    words_unique = []\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        if word not in words_unique:\n",
    "            words_unique.append(word)\n",
    "    for word in words_unique:\n",
    "        word_freq[word] = words.count(word)\n",
    "    return word_freq\n",
    "def finding_freq_of_word_in_doc(word,words):\n",
    "    freq = words.count(word)\n",
    "        \n",
    "def remove_special_characters(text):\n",
    "    regex = re.compile('[^a-zA-Z0-9\\s]')\n",
    "    text_returned = re.sub(regex,'',text)\n",
    "    return text_returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67303b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc1.TXT\n",
      "doc2.TXT\n",
      "doc3.TXT\n",
      "doc4.TXT\n",
      "doc5.TXT\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "dict_global = {}\n",
    "idx = 1\n",
    "files_with_index = {}\n",
    "for(i,doc) in enumerate(document):\n",
    "    print(doc)\n",
    "    fname = doc\n",
    "    file = open(doc , \"r\")\n",
    "    text = file.read()\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(re.compile('\\d'),'',text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(words)>1]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    dict_global.update(finding_all_unique_words_and_freq(words))\n",
    "    files_with_index[idx] = os.path.basename(fname)\n",
    "    idx = idx + 1\n",
    "    \n",
    "unique_words_all = set(dict_global.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04945490",
   "metadata": {},
   "outputs": [],
   "source": [
    "linked_list_data = {}\n",
    "for word in unique_words_all:\n",
    "    linked_list_data[word] = SlinkedList()\n",
    "    linked_list_data[word].head = Node(1,Node)\n",
    "word_freq_in_doc = {}\n",
    "idx = 1\n",
    "for(i,doc) in enumerate(document):\n",
    "    file = open(doc, \"r\")\n",
    "    text = file.read()\n",
    "    text = remove_special_characters(text)\n",
    "    text = re.sub(re.compile('\\d'),'',text)\n",
    "    sentences = sent_tokenize(text)\n",
    "    words = word_tokenize(text)\n",
    "    words = [word for word in words if len(words)>1]\n",
    "    words = [word.lower() for word in words]\n",
    "    words = [word for word in words if word not in Stopwords]\n",
    "    word_freq_in_doc = finding_all_unique_words_and_freq(words)\n",
    "    for word in word_freq_in_doc.keys():\n",
    "        linked_list = linked_list_data[word].head\n",
    "        while linked_list.nextval is not None:\n",
    "            linked_list = linked_list.nextval\n",
    "        linked_list.nextval = Node(idx ,word_freq_in_doc[word])\n",
    "    idx = idx + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1addeceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "database\n",
      "retrieval\n",
      "storage\n",
      "[[1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [0, 1, 0, 0, 1]]\n",
      "[[1, 0, 0, 0, 1], [1, 0, 0, 1, 0], [0, 1, 0, 0, 1]]\n",
      "['doc1.TXT', 'doc5.TXT']\n"
     ]
    }
   ],
   "source": [
    "query = input('Enter your query:')\n",
    "query = word_tokenize(query)\n",
    "connecting_words = []\n",
    "cnt = 1\n",
    "different_words = []\n",
    "for word in query:\n",
    "    if word.lower() != \"and\" and word.lower() != \"or\" and word.lower() != \"not\":\n",
    "        different_words.append(word.lower())\n",
    "    else:\n",
    "        connecting_words.append(word.lower())\n",
    "print(connecting_words)\n",
    "total_files = len(files_with_index)\n",
    "zeroes_and_ones = []\n",
    "zeroes_and_ones_of_all_words = []\n",
    "for word in (different_words):\n",
    "    if word.lower() in unique_words_all:\n",
    "        zeroes_and_ones = [0] * total_files\n",
    "        linkedlist = linked_list_data[word].head\n",
    "        print(word)\n",
    "        while linkedlist.nextval is not None:\n",
    "            zeroes_and_ones[linkedlist.nextval.doc - 1] = 1\n",
    "            linkedlist = linkedlist.nextval\n",
    "        zeroes_and_ones_of_all_words.append(zeroes_and_ones)\n",
    "    else:\n",
    "        print(word,\" not found\")\n",
    "        sys.exit()\n",
    "print(zeroes_and_ones_of_all_words)\n",
    "for word in connecting_words:\n",
    "    word_list1 = zeroes_and_ones_of_all_words[0]\n",
    "    word_list2 = zeroes_and_ones_of_all_words[1]\n",
    "    if word == \"and\":\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"or\":\n",
    "        bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "    elif word == \"not\":\n",
    "        bitwise_op = [not w1 for w1 in word_list2]\n",
    "        bitwise_op = [int(b == True) for b in bitwise_op]\n",
    "        zeroes_and_ones_of_all_words.remove(word_list2)\n",
    "        zeroes_and_ones_of_all_words.remove(word_list1)\n",
    "        bitwise_op = [w1 & w2 for (w1,w2) in zip(word_list1,bitwise_op)]\n",
    "        zeroes_and_ones_of_all_words.insert(0, bitwise_op);\n",
    "        \n",
    "files = []    \n",
    "print(zeroes_and_ones_of_all_words)\n",
    "lis = zeroes_and_ones_of_all_words[0]\n",
    "cnt = 1\n",
    "for index in lis:\n",
    "    if index == 1:\n",
    "        files.append(files_with_index[cnt])\n",
    "    cnt = cnt+1\n",
    "    \n",
    "print(files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d814c5c",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d981f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity:  0.16666666666666666\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "doc_1 =\"Information Retrieval Systems is used with database systems\"\n",
    "doc_2 =\"Information is in Storage Storage\"\n",
    "X=doc_1\n",
    "Y=doc_2\n",
    "\n",
    "# tokenization\n",
    "X_list = word_tokenize(X) \n",
    "Y_list = word_tokenize(Y)\n",
    "  \n",
    "# sw contains the list of stopwords\n",
    "sw = stopwords.words('english') \n",
    "l1 =[];\n",
    "l2 =[]\n",
    "  \n",
    "# remove stop words from the string\n",
    "X_set = {w for w in X_list if not w in sw} \n",
    "Y_set = {w for w in Y_list if not w in sw}\n",
    "  \n",
    "# form a set containing keywords of both strings \n",
    "rvector = X_set.union(Y_set) \n",
    "for w in rvector:\n",
    "    if w in X_set: l1.append(1) # create a vector\n",
    "    else: l1.append(0)\n",
    "    if w in Y_set: l2.append(1)\n",
    "    else: l2.append(0)\n",
    "c = 0\n",
    "  \n",
    "# cosine formula \n",
    "for i in range(len(rvector)):\n",
    "        c+= l1[i]*l2[i]\n",
    "cosine = c / float((sum(l1)*sum(l2))*0.5)\n",
    "print(\"Cosine similarity: \", cosine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcf0200",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9999696",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3404255319148936"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dice_coefficient(a, b):\n",
    "    \"\"\"dice coefficient 2nt/(na + nb).\"\"\"\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    if len(a) == 1:  a=a+u'.'\n",
    "    if len(b) == 1:  b=b+u'.'\n",
    "    \n",
    "    a_bigram_list=[]\n",
    "    for i in range(len(a)-1):\n",
    "      a_bigram_list.append(a[i:i+2])\n",
    "    b_bigram_list=[]\n",
    "    for i in range(len(b)-1):\n",
    "      b_bigram_list.append(b[i:i+2])\n",
    "      \n",
    "    a_bigrams = set(a_bigram_list)\n",
    "    b_bigrams = set(b_bigram_list)\n",
    "    overlap = len(a_bigrams & b_bigrams)\n",
    "    dice_coeff = overlap * 2.0/(len(a_bigrams) + len(b_bigrams))\n",
    "    return dice_coeff\n",
    "\n",
    "\"\"\" duplicate bigrams in a word should be counted distinctly\n",
    "(per discussion), otherwise 'AA' and 'AAAA' would have a \n",
    "dice coefficient of 1...\n",
    "\"\"\"\n",
    "def dice_coefficient(a,b):\n",
    "    if not len(a) or not len(b): return 0.0\n",
    "    \"\"\" quick case for true duplicates \"\"\"\n",
    "    if a == b: return 1.0\n",
    "    \"\"\" if a != b, and a or b are single chars, then they can't possibly match \"\"\"\n",
    "    if len(a) == 1 or len(b) == 1: return 0.0\n",
    "    \n",
    "    \"\"\" use python list comprehension, preferred over list.append() \"\"\"\n",
    "    a_bigram_list = [a[i:i+2] for i in range(len(a)-1)]\n",
    "    b_bigram_list = [b[i:i+2] for i in range(len(b)-1)]\n",
    "    \n",
    "    a_bigram_list.sort()\n",
    "    b_bigram_list.sort()\n",
    "    \n",
    "    # assignments to save function calls\n",
    "    lena = len(a_bigram_list)\n",
    "    lenb = len(b_bigram_list)\n",
    "    # initialize match counters\n",
    "    matches = i = j = 0\n",
    "    while (i < lena and j < lenb):\n",
    "        if a_bigram_list[i] == b_bigram_list[j]:\n",
    "            matches += 2\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif a_bigram_list[i] < b_bigram_list[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    score = float(matches)/float(lena + lenb)\n",
    "    return score\n",
    "\n",
    "dice_coefficient(\"Digital Speech systems can be used in Synthesis and Systems\", \"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39909433",
   "metadata": {},
   "source": [
    "# Question 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcaad675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Jaccard_Similarity(doc1, doc2): \n",
    "    \n",
    "    # List the unique words in a document\n",
    "    words_doc1 = set(doc1.split()) \n",
    "    words_doc2 = set(doc2.split())\n",
    "    \n",
    "    # Find the intersection of words list of doc1 & doc2\n",
    "    intersection = words_doc1.intersection(words_doc2)\n",
    "\n",
    "    # Find the union of words list of doc1 & doc2\n",
    "    union = words_doc1.union(words_doc2)\n",
    "        \n",
    "    # Calculate Jaccard similarity score \n",
    "    # using length of intersection set divided by length of union set\n",
    "    return float(len(intersection)) / len(union)\n",
    "\n",
    "doc_1 = \"Speech Filtering, Speech Retrieval systems are applications of Information Retrieval \"\n",
    "doc_2 = \"Database Management system is used for storage storage\"\n",
    "\n",
    "Jaccard_Similarity(doc_1,doc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25866078",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
